{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598008897786",
   "display_name": "Python 3.7.6 64-bit ('env': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a simple jupyter to study stuff (embedding and attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DatasetCreator'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-30ae7aead3e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mDatasetCreator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetCreator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetCreator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meq_learner_architectures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvconv_PoseNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DatasetCreator'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from DatasetCreator.DatasetCreator import DatasetCreator\n",
    "from sympy import sin, Symbol, log, exp \n",
    "from eq_learner_architectures import convconv_PoseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self):\n",
    "        layer = torch.nn.Embedding(3,2)\n",
    "        final_layer = nn.Linear(2,1)\n",
    "        activation_fun = nn.Sigmoid(final)\n",
    "    def __call__(inp):\n",
    "        partial = layer(sol)\n",
    "        final = nn.Sigmoid(final_layer(partial))\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder Point Cloud data\n",
    "x = Symbol('x')\n",
    "basis_functions = [x,sin,log,exp]\n",
    "support = np.arange(-2,2, 0.01)\n",
    "\n",
    "fun_generator = DatasetCreator(basis_functions, max_linear_terms=2,max_binomial_terms=3,max_N_terms=0, max_compositions=2, division_on=False)\n",
    "input_network, output_network =  fun_generator.generate_batch(support,20)\n",
    "input_network = np.nan_to_num(input_network)\n",
    "first_encoder = convconv_PoseNet.PoseNetEncoder(len(support),2)\n",
    "input_tensor = torch.tensor(input_network).float()\n",
    "emb = first_encoder(input_tensor) #Output Batch, hidden dim = 256, feature 1\n",
    "emb = emb.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.tensor([[1],[2],[3],[0]], dtype=torch.long)\n",
    "sol =  torch.tensor([1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The two layers\n",
    "layer = torch.nn.Embedding(4,2)\n",
    "final_layer = nn.Linear(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[-1.5408,  0.1991]],\n\n        [[ 0.5197, -1.3244]],\n\n        [[ 1.3603, -1.0763]],\n\n        [[-0.1120, -0.2936]]], grad_fn=<EmbeddingBackward>)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "layer(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder architecture. Data assumptions:\n",
    "<ol>\n",
    "<li>Target sequence of 5 differnt tokens with length of 6. 0 token represent a padding token\n",
    "<li>Batch dimensions of 2\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1, 2, 4, 3, 2, 0],\n        [1, 2, 3, 1, 1, 1]])\ntensor([[0, 1, 2, 3, 4, 5],\n        [0, 1, 2, 3, 4, 5]])\nNumber of tokens output: 5\nSequence Length 6\n"
    }
   ],
   "source": [
    "src = torch.tensor([[1,2,4,3,2,0],[1,2,3,1,1,1]],dtype=torch.long)\n",
    "pos = torch.arange(0, src.shape[1]).unsqueeze(0).repeat(2, 1)\n",
    "n_tokens = int(src.max()+1)\n",
    "length_sequence = int(src.shape[1])\n",
    "print(src)\n",
    "print(pos)\n",
    "print(\"Number of tokens output:\", n_tokens)\n",
    "print(\"Sequence Length\", length_sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder Architecture. Parameters assumptions:\n",
    "<ol>\n",
    "<li> Embedding with an embedding dimension of 2\n",
    "<li> Hidden Dimension twice as big as the embedding. There is an linear layer mapping the embedding to the Linear Layer\n",
    "<li> 10 Convolutionals layers with kernel size of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Batch, Sequence Length, Embedding: torch.Size([2, 6, 2]) \n\ntensor([[[-0.3273,  0.1837],\n         [ 0.3341,  1.9240],\n         [ 0.1831,  1.0833],\n         [-1.8204,  0.0126],\n         [ 0.3341,  1.9240],\n         [-1.5161, -0.8355]],\n\n        [[-0.3273,  0.1837],\n         [ 0.3341,  1.9240],\n         [-1.8204,  0.0126],\n         [-0.3273,  0.1837],\n         [-0.3273,  0.1837],\n         [-0.3273,  0.1837]]], grad_fn=<EmbeddingBackward>)\ntorch.Size([2, 6, 2])\ntensor([[[ 0.2376,  0.4152],\n         [ 3.0028, -0.5745],\n         [ 0.2114, -0.2161],\n         [-0.2862, -0.2971],\n         [-0.1592,  0.8139],\n         [ 0.7925,  0.1732]],\n\n        [[ 0.2376,  0.4152],\n         [ 3.0028, -0.5745],\n         [ 0.2114, -0.2161],\n         [-0.2862, -0.2971],\n         [-0.1592,  0.8139],\n         [ 0.7925,  0.1732]]], grad_fn=<EmbeddingBackward>)\n"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 2\n",
    "\n",
    "tok_embedded = nn.Embedding(n_tokens,EMBEDDING_DIM)(src)\n",
    "pos_embedded = nn.Embedding(length_sequence,EMBEDDING_DIM)(pos)\n",
    "print(\"Batch, Sequence Length, Embedding: {} \\n\".format(tok_embedded.shape))\n",
    "print(tok_embedded)\n",
    "print(pos_embedded.shape)\n",
    "print(pos_embedded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[-0.1196,  0.0000],\n         [ 4.4492,  1.7994],\n         [ 0.0000,  0.0000],\n         [-2.8087, -0.3794],\n         [ 0.0000,  3.6506],\n         [-0.9648, -0.8831]],\n\n        [[-0.1196,  0.7985],\n         [ 4.4492,  0.0000],\n         [-0.0000, -0.0000],\n         [-0.8180, -0.1512],\n         [-0.6487,  1.3301],\n         [ 0.0000,  0.4758]]], grad_fn=<MulBackward0>)\n"
    }
   ],
   "source": [
    "embedded = nn.Dropout(0.25)(tok_embedded + pos_embedded) #No influence at evaluation, but default is training\n",
    "print(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Batch 2, Length sequence 6, Hidden state 4\ntensor([[[-0.7705,  0.1613,  0.4604,  0.2774],\n         [ 1.3464,  1.8496, -2.1123, -2.4623],\n         [-0.7064,  0.1867,  0.3935,  0.2236],\n         [-2.1415, -0.5609,  1.9675,  1.6305],\n         [-1.3780,  1.6444,  0.3553, -1.1694],\n         [-1.0609, -0.3707,  0.9421,  0.9942]],\n\n        [[-0.9174,  0.4802,  0.4520, -0.0273],\n         [ 1.6774,  1.1311, -2.0935, -1.7757],\n         [-0.7064,  0.1867,  0.3935,  0.2236],\n         [-1.1169, -0.0473,  0.8524,  0.6489],\n         [-1.2987,  0.5801,  0.7422,  0.0076],\n         [-0.7940,  0.3767,  0.3885,  0.0421]]], grad_fn=<AddBackward0>)\n\n \n Preparing for CNN \n\nBatch 2, Hidden sequence 4, Length state 6\ntensor([[[-0.7705,  1.3464, -0.7064, -2.1415, -1.3780, -1.0609],\n         [ 0.1613,  1.8496,  0.1867, -0.5609,  1.6444, -0.3707],\n         [ 0.4604, -2.1123,  0.3935,  1.9675,  0.3553,  0.9421],\n         [ 0.2774, -2.4623,  0.2236,  1.6305, -1.1694,  0.9942]],\n\n        [[-0.9174,  1.6774, -0.7064, -1.1169, -1.2987, -0.7940],\n         [ 0.4802,  1.1311,  0.1867, -0.0473,  0.5801,  0.3767],\n         [ 0.4520, -2.0935,  0.3935,  0.8524,  0.7422,  0.3885],\n         [-0.0273, -1.7757,  0.2236,  0.6489,  0.0076,  0.0421]]],\n       grad_fn=<PermuteBackward>)\n"
    }
   ],
   "source": [
    "HIDDEN_DIM = 4\n",
    "hidden_output = nn.Linear(2,HIDDEN_DIM)(embedded)\n",
    "print(\"Batch {}, Length sequence {}, Hidden state {}\".format(hidden_output.shape[0],hidden_output.shape[1],hidden_output.shape[2]))\n",
    "print(hidden_output)\n",
    "\n",
    "print(\"\\n \\n Preparing for CNN \\n\")\n",
    "conv_input = hidden_output.permute(0, 2, 1) \n",
    "print(\"Batch {}, Hidden sequence {}, Length state {}\".format(conv_input.shape[0],conv_input.shape[1],conv_input.shape[2]))\n",
    "print(conv_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Time\n",
    "For the seek of learning we decompose the for loop in two parts. One cycle analyzed on its own, the rest normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[ 4.2386e-01,  1.4308e+00,  5.5319e-01,  2.7419e-01,  2.2194e-01,\n          -5.7322e-01],\n         [ 1.3442e+00,  1.0927e+00, -1.7025e+00, -2.3416e-01,  1.5453e+00,\n          -1.3018e+00],\n         [ 8.1805e-01,  2.6739e+00,  1.4731e+00,  7.6560e-01,  4.2511e-01,\n          -6.1928e-01],\n         [ 9.9371e-01, -7.9609e-01, -5.6210e-01,  5.4903e-01,  1.4189e+00,\n           1.4925e+00]],\n\n        [[ 4.5974e-01,  1.4308e+00,  5.6545e-01, -2.1756e-04,  2.3541e-02,\n           3.1811e-02],\n         [ 1.3131e+00,  1.0927e+00, -4.2932e-01, -3.9743e-03,  3.8912e-02,\n          -6.3682e-01],\n         [ 8.8725e-01,  2.6739e+00,  1.3193e+00,  2.3753e-01,  2.7456e-01,\n           3.8294e-01],\n         [ 9.2018e-01, -7.9609e-01, -2.2515e-02,  1.1079e+00,  1.0871e+00,\n           7.7609e-01]]], grad_fn=<PermuteBackward>)\ntensor([[[ 5.6514e-01,  1.9078e+00,  7.3758e-01,  3.6559e-01,  0.0000e+00,\n          -7.6429e-01],\n         [ 1.7923e+00,  1.4569e+00, -2.2700e+00, -3.1221e-01,  2.0603e+00,\n          -0.0000e+00],\n         [ 1.0907e+00,  3.5651e+00,  1.9642e+00,  1.0208e+00,  5.6682e-01,\n          -8.2571e-01],\n         [ 1.3250e+00, -0.0000e+00, -7.4947e-01,  0.0000e+00,  0.0000e+00,\n           1.9900e+00]],\n\n        [[ 0.0000e+00,  0.0000e+00,  7.5393e-01, -2.9008e-04,  3.1387e-02,\n           4.2415e-02],\n         [ 1.7508e+00,  1.4569e+00, -0.0000e+00, -5.2991e-03,  5.1883e-02,\n          -8.4909e-01],\n         [ 1.1830e+00,  3.5651e+00,  0.0000e+00,  3.1671e-01,  3.6609e-01,\n           5.1059e-01],\n         [ 1.2269e+00, -1.0615e+00, -3.0020e-02,  1.4772e+00,  1.4495e+00,\n           1.0348e+00]]], grad_fn=<MulBackward0>)\n"
    }
   ],
   "source": [
    "KERNEL_SIZE = 3\n",
    "PADDING_INDEX = 0\n",
    "print(conv_input)\n",
    "conv_input_dropped = nn.Dropout(0.25)(conv_input)\n",
    "print(conv_input_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Preparing Padding\ntensor([[[0., 0.],\n         [0., 0.],\n         [0., 0.],\n         [0., 0.]],\n\n        [[0., 0.],\n         [0., 0.],\n         [0., 0.],\n         [0., 0.]]])\ntensor([[[0., 0.],\n         [0., 0.],\n         [0., 0.],\n         [0., 0.]],\n\n        [[0., 0.],\n         [0., 0.],\n         [0., 0.],\n         [0., 0.]]])\n"
    }
   ],
   "source": [
    "print(\"Preparing Padding\")\n",
    "preparing_padding_cake = torch.zeros(2, HIDDEN_DIM, KERNEL_SIZE-1)\n",
    "print(preparing_padding_cake)\n",
    "padding = preparing_padding_cake.fill_(PADDING_INDEX) #In our case this command doesn't do much, as the padding index is zero\n",
    "print(padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Before Convolutions \n\ntorch.Size([2, 4, 8]) \n\ntensor([[[ 0.0000e+00,  0.0000e+00,  5.6514e-01,  1.9078e+00,  7.3758e-01,\n           3.6559e-01,  0.0000e+00, -7.6429e-01],\n         [ 0.0000e+00,  0.0000e+00,  1.7923e+00,  1.4569e+00, -2.2700e+00,\n          -3.1221e-01,  2.0603e+00, -0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  1.0907e+00,  3.5651e+00,  1.9642e+00,\n           1.0208e+00,  5.6682e-01, -8.2571e-01],\n         [ 0.0000e+00,  0.0000e+00,  1.3250e+00, -0.0000e+00, -7.4947e-01,\n           0.0000e+00,  0.0000e+00,  1.9900e+00]],\n\n        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.5393e-01,\n          -2.9008e-04,  3.1387e-02,  4.2415e-02],\n         [ 0.0000e+00,  0.0000e+00,  1.7508e+00,  1.4569e+00, -0.0000e+00,\n          -5.2991e-03,  5.1883e-02, -8.4909e-01],\n         [ 0.0000e+00,  0.0000e+00,  1.1830e+00,  3.5651e+00,  0.0000e+00,\n           3.1671e-01,  3.6609e-01,  5.1059e-01],\n         [ 0.0000e+00,  0.0000e+00,  1.2269e+00, -1.0615e+00, -3.0020e-02,\n           1.4772e+00,  1.4495e+00,  1.0348e+00]]], grad_fn=<CatBackward>)\nAfter Convolution \n\ntorch.Size([2, 8, 6]) \n\ntensor([[[ 0.2275, -0.8310, -1.9746,  0.5754,  0.3114, -0.2350],\n         [ 0.0258, -0.0628, -0.9638, -0.7149,  0.9464,  0.3711],\n         [-0.0946,  0.0730, -1.4545,  0.6838,  0.9814, -0.8033],\n         [ 0.3635, -0.4294, -1.4220, -1.1149,  0.3878,  0.7623],\n         [ 0.0547,  0.4045,  1.1897,  0.5597, -0.3781,  0.7276],\n         [-0.0122,  1.1299,  1.5268, -0.3665,  0.0629,  0.7622],\n         [-0.5826, -0.4598,  0.3041,  0.6045, -0.5074, -0.3183],\n         [-0.8179, -1.0436,  0.8268,  0.7134, -0.5151, -0.7044]],\n\n        [[ 0.1169, -1.0673, -0.7517,  1.0151,  0.1938, -0.2319],\n         [-0.0673, -0.3565, -0.3914,  0.1326, -0.3762, -0.5818],\n         [-0.2295, -0.3517, -1.7127,  0.5225,  0.0637, -0.2582],\n         [ 0.3852, -0.4912, -0.5011, -0.2093, -0.0916, -0.6117],\n         [ 0.0831,  0.3813,  1.1300,  1.0426,  0.3330,  0.4651],\n         [ 0.0909,  1.4123,  0.5513,  0.0716,  0.5216,  1.1960],\n         [-0.4019, -0.1765, -1.5197,  0.4262,  0.3865,  0.1586],\n         [-0.8117, -0.9643,  0.2780, -0.1911, -0.5434,  0.0383]]],\n       grad_fn=<SqueezeBackward1>)\n"
    }
   ],
   "source": [
    "padded_conv_input = torch.cat((padding, conv_input_dropped), dim = 2)\n",
    "print(\"Before Convolutions \\n\")\n",
    "print(padded_conv_input.shape, \"\\n\")\n",
    "print(padded_conv_input)\n",
    "print(\"After Convolution \\n\")\n",
    "conved = nn.Conv1d(in_channels = HIDDEN_DIM, out_channels = 2 * HIDDEN_DIM,  kernel_size = KERNEL_SIZE)(padded_conv_input)\n",
    "print(conved.shape, \"\\n\")\n",
    "print(conved)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far everything was linear. GLU activation function is the game changer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([2, 4, 6])\n\n tensor([[[ 0.1169, -0.4984, -1.5139,  0.3662,  0.1266, -0.1585],\n         [ 0.0128, -0.0475, -0.7918, -0.2927,  0.4881,  0.2531],\n         [-0.0339,  0.0283, -0.8370,  0.4422,  0.3688, -0.3382],\n         [ 0.1113, -0.1118, -0.9893, -0.7483,  0.1450,  0.2522]],\n\n        [[ 0.0609, -0.6341, -0.5681,  0.7505,  0.1129, -0.1424],\n         [-0.0352, -0.2867, -0.2483,  0.0687, -0.2361, -0.4467],\n         [-0.0920, -0.1604, -0.3074,  0.3161,  0.0379, -0.1393],\n         [ 0.1185, -0.1356, -0.2852, -0.0947, -0.0337, -0.3117]]],\n       grad_fn=<GluBackward>)\n"
    }
   ],
   "source": [
    "glu_conved = F.glu(conved, dim = 1)\n",
    "print(glu_conved.shape)\n",
    "print(\"\\n\", glu_conved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the Attention mechanism part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[ 0.1169,  0.0128, -0.0339,  0.1113],\n         [-0.4984, -0.0475,  0.0283, -0.1118],\n         [-1.5139, -0.7918, -0.8370, -0.9893],\n         [ 0.3662, -0.2927,  0.4422, -0.7483],\n         [ 0.1266,  0.4881,  0.3688,  0.1450],\n         [-0.1585,  0.2531, -0.3382,  0.2522]],\n\n        [[ 0.0609, -0.0352, -0.0920,  0.1185],\n         [-0.6341, -0.2867, -0.1604, -0.1356],\n         [-0.5681, -0.2483, -0.3074, -0.2852],\n         [ 0.7505,  0.0687,  0.3161, -0.0947],\n         [ 0.1129, -0.2361,  0.0379, -0.0337],\n         [-0.1424, -0.4467, -0.1393, -0.3117]]], grad_fn=<PermuteBackward>)\n\n\ntensor([[[-0.3505,  0.4517],\n         [-0.0373,  0.1732],\n         [ 1.2832, -0.3942],\n         [-0.1859, -0.0703],\n         [-0.7333,  0.5150],\n         [-0.3043,  0.6273]],\n\n        [[-0.2883,  0.4411],\n         [ 0.2026,  0.1014],\n         [ 0.2558,  0.1283],\n         [-0.6725,  0.4418],\n         [-0.2060,  0.2789],\n         [ 0.1419,  0.0931]]], grad_fn=<AddBackward0>)\n"
    }
   ],
   "source": [
    "conved_back = glu_conved.permute(0, 2, 1)\n",
    "print(conved_back)\n",
    "conved_emb = nn.Linear(HIDDEN_DIM, EMBEDDING_DIM)(conved_back)\n",
    "print(\"\\n\")\n",
    "print(conved_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[-0.2479, -1.4382],\n         [-2.3309, -2.2747],\n         [-0.3814,  0.9833],\n         [-0.3211, -0.0497],\n         [-0.0068, -1.4203],\n         [ 1.1133,  2.2982]],\n\n        [[-0.2933, -1.4457],\n         [-2.1613, -2.3255],\n         [-0.7156,  0.0317],\n         [ 0.0161,  0.3204],\n         [ 0.3077,  0.1398],\n         [ 0.3129,  0.6964]]], grad_fn=<MulBackward0>)\n"
    }
   ],
   "source": [
    " SCALE = torch.sqrt(torch.FloatTensor([0.5]))\n",
    " combined = (conved_emb + embedded) * SCALE\n",
    " print(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'encoder_conved' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-5850eba78aab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_conved\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder_conved' is not defined"
     ]
    }
   ],
   "source": [
    "energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[2., 2.],\n        [2., 2.]])"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "a = torch.ones((2,2))\n",
    "b = torch.ones((2,2))\n",
    "torch.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1., 1.],\n        [1., 1.]])"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "torch.ones((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-804fe27bf00e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mCONV_LAYERS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONV_LAYERS\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mconv_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "CONV_LAYERS = 10\n",
    "for i, conv in enumerate(CONV_LAYERS-1):\n",
    "    conv_input = nn.Dropout(0.25)(conv_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
    "    \n",
    "    #embedded = [batch size, trg len, emb dim]\n",
    "    #conved = [batch size, hid dim, trg len]\n",
    "    #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
    "    \n",
    "    #permute and convert back to emb dim\n",
    "    conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
    "    \n",
    "    #conved_emb = [batch size, trg len, emb dim]\n",
    "    \n",
    "    combined = (conved_emb + embedded) * self.scale\n",
    "    \n",
    "    #combined = [batch size, trg len, emb dim]\n",
    "            \n",
    "    energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
    "    \n",
    "    #energy = [batch size, trg len, src len]\n",
    "    \n",
    "    attention = F.softmax(energy, dim=2)\n",
    "    \n",
    "    #attention = [batch size, trg len, src len]\n",
    "        \n",
    "    attended_encoding = torch.matmul(attention, encoder_combined)\n",
    "    \n",
    "    #attended_encoding = [batch size, trg len, emd dim]\n",
    "    \n",
    "    #convert from emb dim -> hid dim\n",
    "    attended_encoding = self.attn_emb2hid(attended_encoding)\n",
    "    \n",
    "    #attended_encoding = [batch size, trg len, hid dim]\n",
    "    \n",
    "    #apply residual connection\n",
    "    attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
    "    \n",
    "    #attended_combined = [batch size, hid dim, trg len]\n",
    "    \n",
    "    return attention, attended_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}