{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis Recognition (Symbol-Supervised Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler, minmax_scale\n",
    "import torch.nn as nn\n",
    "from Data_reading import*\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dawnload and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Symbol('x')\n",
    "\n",
    "support = support_reading()\n",
    "X_lin, Y_complexity_lin, GT_function_lin ,labels_one_hot_lin = read_data(\n",
    "    support, dir = r\"C:\\Users\\lbg\\OneDrive - CSEM S.A\\Bureau\\Pytorch\\NEW_EQ_LEARN\\Data\\Binomials\")\n",
    "\n",
    "X_lin = minmax_scale(X_lin,axis = 1)\n",
    "\n",
    "X_lin_train, labels_one_hot_lin_train, GT_function_lin_train = X_lin[:8000], labels_one_hot_lin[:8000,:-1], GT_function_lin[:8000]\n",
    "X_lin_test, labels_one_hot_lin_test, GT_function_lin_test = X_lin[8000:], labels_one_hot_lin[8000:,:-1], GT_function_lin[8000:]\n",
    "\n",
    "labels_one_hot_lin_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lin_train = np.expand_dims(X_lin_train,1)\n",
    "X_lin_test = np.expand_dims(X_lin_test,1)\n",
    "print(X_lin_test.shape)\n",
    "print(X_lin_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Iterator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset,  labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.label = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        data = self.dataset[idx, :].astype('float32')\n",
    "        label1 = self.label[idx, :]\n",
    "        data = torch.from_numpy(data)\n",
    "        label1 = torch.from_numpy(label1.astype('float32'))\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data,label1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset(X_lin_train, labels_one_hot_lin_train)\n",
    "dataset_test_h = dataset(X_lin_test, labels_one_hot_lin_test)\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "valid_size = 0.20\n",
    "num_train = len(dataset_train)\n",
    "num_test_h = len(dataset_test_h)\n",
    "indices = list(range(num_train))\n",
    "test_idx_h = list(range(num_test_h))\n",
    "np.random.shuffle(test_idx_h)\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "test_sampler_h = SubsetRandomSampler(test_idx_h)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=0)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=0)\n",
    "test_loader_h = torch.utils.data.DataLoader(dataset_test_h, batch_size=batch_size, \n",
    "    sampler=test_sampler_h, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 9, 6, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=3)\n",
    "        self.conv2 = nn.Conv1d(9, 9, 6, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=3)\n",
    "        self.conv3 = nn.Conv1d(9, 9, 6, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=3)\n",
    "        self.conv4 = nn.Conv1d(9, 9, 6, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=3)\n",
    "        self.fc1 = nn.Linear(3*2958, 500)\n",
    "        self.fc2 = nn.Linear(500, 7)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.dropout5 = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu((self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu((self.conv2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu((self.conv3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu((self.conv4(x)))\n",
    "        x = self.dropout4(x)\n",
    "        x = x.view(-1, x.shape[2]*9)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout5(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a complete NN\n",
    "model = Net()\n",
    "model.float()\n",
    "print(model)\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay = 0)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "\n",
    "\n",
    "\n",
    "# track change in validation loss\n",
    "valid_loss_min = np.Inf \n",
    "\n",
    "\n",
    "print('Do you want to retrain the model?')\n",
    "resp = input()\n",
    "if resp == 'yes':\n",
    "    print('Do you want to save the model parameters?')\n",
    "    save = input()\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            print(output)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for data, target in valid_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            #_, pred = torch.max(output, 1)   \n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.sampler)\n",
    "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))\n",
    "        \n",
    "        if save == 'yes':\n",
    "            # save model if validation loss has decreased\n",
    "            if valid_loss <= valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss))\n",
    "                torch.save(model.state_dict(), r\"C:\\Users\\lbg\\OneDrive - CSEM S.A\\Bureau\\Pytorch\\NEW_EQ_LEARN\\Saved_Models\\CNN1.pt\")\n",
    "                valid_loss_min = valid_loss\n",
    "else:\n",
    "    print('Model not trained')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\lbg\\OneDrive - CSEM S.A\\Bureau\\Pytorch\\NEW_EQ_LEARN\\Saved_Models\\CNN1.pt\"))\n",
    "Y_hat_test = model(torch.from_numpy(X_lin_test.astype('float32')))\n",
    "Y_hat_val = model(torch.from_numpy(X_lin_train[valid_idx].astype('float32')))\n",
    "Y_hat_tr = model(torch.from_numpy(X_lin_train[train_idx].astype('float32')))\n",
    "Y_hat_train = model(torch.from_numpy(X_lin_train.astype('float32')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Symbol('x')\n",
    "basis_functions = np.array([sin(x),exp(x), x, exp(-x), 1/x, sin(x**2), exp(-x**2),1])\n",
    "yb = torch.Tensor([[lambdify(x,basis_functions[j])(i) for i in support] for j in range(len(basis_functions)-1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = torch.transpose(torch.Tensor(X_lin_train),1,2)[:100]\n",
    "x = torch.eye(7)\n",
    "x = x.reshape((1, 7, 7))\n",
    "y = x.repeat(100, 1, 1)\n",
    "a = torch.nn.Threshold(0.5,0)\n",
    "yy = a(Y_hat_test[:100])\n",
    "s = torch.unsqueeze(yy,1)\n",
    "q = s*y\n",
    "c = torch.bmm(torch.unsqueeze(yb.T,0).repeat(100,1,1),q)\n",
    "d = torch.bmm(torch.transpose(c,1,2),c) + 0.00001*torch.eye(7)\n",
    "e = torch.inverse(d)\n",
    "f = torch.bmm(e,torch.transpose(c,1,2))\n",
    "print(torch.bmm(f,ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at  perfromances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15,150,120,190,51,173\n",
    "%matplotlib notebook\n",
    "\n",
    "temp = Y_hat_test>0.5\n",
    "Y_hat_test = temp.numpy().astype(int)\n",
    "temp = Y_hat_train>0.5\n",
    "Y_hat_train = temp.numpy().astype(int)\n",
    "\n",
    "print('Overall Accuracy')\n",
    "print(1-sum([False in (labels_one_hot_lin_test[i] == Y_hat_test[i]) for i in range(len(Y_hat_test))])/len(Y_hat_test))\n",
    "print(1-sum([False in (labels_one_hot_lin_train[i] == Y_hat_train[i]) for i in range(len(Y_hat_train))])/len(Y_hat_train))\n",
    "print()\n",
    "\n",
    "basis_functions = np.array([sin(x),exp(x), x, exp(-x), 1/x, sin(x**2), exp(-x**2),1])\n",
    "sample = 211\n",
    "print('True Equation:')\n",
    "print(GT_function_lin_test[sample])\n",
    "print()\n",
    "print('Prediction')\n",
    "print(Y_hat_test[sample])\n",
    "print()\n",
    "print('Ground Truth')\n",
    "print(labels_one_hot_lin_test[sample])\n",
    "y = [lambdify(x,GT_function_lin_test[sample])(i) for i in support]\n",
    "plt.plot(support,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.plot(support,X_lin_test[493,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-basis Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Set')\n",
    "for i in range(7):\n",
    "    print(sum(labels_one_hot_lin_test[:,i] == Y_hat_test[:,i])/len(Y_hat_test))\n",
    "\n",
    "print()    \n",
    "    \n",
    "\n",
    "print('Training Set')    \n",
    "for i in range(7):\n",
    "    print(sum(labels_one_hot_lin_train[:,i] == Y_hat_train[:,i])/len(Y_hat_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "sums = [sum(labels_one_hot_lin_test[:,i]) for i in range(8)]\n",
    "confusion = multilabel_confusion_matrix(labels_one_hot_lin_test, Y_hat_test)\n",
    "for i in range(8):\n",
    "    print(confusion[i]/sums[i])\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
