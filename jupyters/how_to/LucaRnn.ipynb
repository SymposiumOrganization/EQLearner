{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq EqLearn -- First Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import sympy\n",
    "from sympy import lambdify\n",
    "from eq_learner.DatasetCreator import DatasetCreator\n",
    "from eq_learner.processing import tokenization\n",
    "from sympy import sin, Symbol, log, exp \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchtext.datasets \n",
    "from torchtext.data import Field, BucketIterator\n",
    "import matplotlib.pyplot as plt\n",
    "#from eq_learner.processing import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets\n",
    "As a first experiment we will create a dataset with all the targets padded to same length. Additionally, we discard equation yielding too large values associated with the fixed support. This is to apply a scaling and have data from 0 tpo 1. \n",
    "Next stesps should create variable length output and batch them according to their length so that the amount of padding is minimized. See torchtext.dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU as standard device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ckasses handling batches generation and train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(Dataset):\n",
    "    r\"\"\"Dataset wrapping tensors.\n",
    "\n",
    "    Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "\n",
    "    Arguments:\n",
    "        *tensors (Tensor): tensors that have the same size of the first dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *tensors):\n",
    "        self.tensors = tensors\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(tensor[index].cuda() for tensor in self.tensors)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "    \n",
    "def dataset_loader(train_dataset,test_dataset, batch_size = 1024, valid_size = 0.20):\n",
    "    num_train = len(train_dataset)\n",
    "    num_test_h = len(test_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    test_idx_h = list(range(num_test_h))\n",
    "    np.random.shuffle(test_idx_h)\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "        sampler=train_sampler, num_workers=0)\n",
    "    valid_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "        sampler=valid_sampler, num_workers=0)\n",
    "    test_loader_h = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, \n",
    "        shuffle=False, num_workers=0)\n",
    "    return train_loader, valid_loader, test_loader_h, valid_idx, train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sin(x)+sin(exp(2*x)+1)\n",
      "[12  2  5  1  6  9  2  5  3  5 15  8  1  6  9 14  6 13]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1cece863358>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scale = True\n",
    "scaler = MinMaxScaler()\n",
    "x = Symbol('x')\n",
    "basis_functions = [x,sin,log,exp]\n",
    "support = np.arange(0.1,3.1,0.1)\n",
    "fun_generator = DatasetCreator(basis_functions,max_linear_terms=1, max_binomial_terms=1,max_compositions=1,max_N_terms=1,division_on=False,  random_terms=True, constants_enabled = True, constant_intervals_ext=[(-10,1),(1,10)], constant_intervals_int = [(1,3)])\n",
    "string, dictionary = fun_generator.generate_batch(support,1, X_noise=False, Y_noise=0)\n",
    "print(tokenization.get_string(tokenization.pipeline(dictionary)[0]))\n",
    "print(tokenization.pipeline(dictionary)[0])\n",
    "plt.plot(string[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sympy.core.symbol.Symbol"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.Symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equations sampling and numerical scaling\n",
    "Need to test if it's better to normalize across time or across data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "scale = True\n",
    "scaler = MinMaxScaler()\n",
    "x = Symbol('x')\n",
    "basis_functions = [x,sin,log,exp]\n",
    "support = np.arange(0.1,3.1,0.1)\n",
    "fun_generator = DatasetCreator(basis_functions,max_linear_terms=1, max_binomial_terms=1,max_compositions=1,max_N_terms=0,division_on=False,  random_terms=True, constants_enabled = True, constant_intervals_ext=[(-3,1),(1,3)], constant_intervals_int = [(1,3)])\n",
    "x_train = []\n",
    "y_train = []\n",
    "cnt = 0\n",
    "cond = True\n",
    "# while cond == True:\n",
    "#     string, dictionary =  fun_generator.generate_batch(support,1, X_noise=False, Y_noise=0)\n",
    "#     if np.all(string[0][1] == 0) == False:\n",
    "#         if np.max(string[0][1])<1000 and np.min(string[0][1])>-1000 and tokenization.get_string(tokenization.pipeline(dictionary)[0])[-1] != '+': \n",
    "#             x_train.append(string[0][1])\n",
    "#             y_train.append(torch.Tensor((tokenization.pipeline(dictionary)[0])))\n",
    "# #             print(tokenization.get_string(tokenization.pipeline(dictionary)[0]))\n",
    "#             cnt+=1\n",
    "#             print(cnt)\n",
    "#     if cnt == 500000:\n",
    "#         cond = False\n",
    "# scaler.fit(np.array(x_train).T)\n",
    "# x_train_n = scaler.transform(np.array(x_train).T)\n",
    "# x_train_n = torch.Tensor(x_train_n)\n",
    "# l = [len(y) for y in y_train]\n",
    "# q = np.max(l)\n",
    "# y_train_p = torch.zeros(len(y_train),q)\n",
    "# for i,y in enumerate(y_train):\n",
    "#     y_train_p[i,:] = torch.cat([y,torch.zeros(q-y.shape[0])])\n",
    "# train_data = TensorDataset(x_train_n.T,y_train_p.long())\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "cnt = 0\n",
    "cond = True\n",
    "while cond == True:\n",
    "    string, dictionary =  fun_generator.generate_batch(support,1, X_noise=False, Y_noise=0)\n",
    "    if np.all(string[0][1] == 0) == False:\n",
    "        if np.max(string[0][1])<1000 and np.min(string[0][1])>-1000 and tokenization.get_string(tokenization.pipeline(dictionary)[0])[-1] != '+': \n",
    "            x_test.append(string[0][1])\n",
    "            y_test.append(torch.Tensor((tokenization.pipeline(dictionary)[0])))\n",
    "            #print(tokenization.get_string(tokenization.pipeline(dictionary)[0]))\n",
    "            cnt+=1\n",
    "            print(cnt)\n",
    "    if cnt == 500:\n",
    "        cond = False\n",
    "scaler = MinMaxScaler()\n",
    "x_test_n = scaler.fit_transform(np.array(x_test).T)\n",
    "x_test_n = torch.Tensor(x_test_n)\n",
    "l = [len(y) for y in y_test]\n",
    "q = np.max(l)\n",
    "y_test_p = torch.zeros(len(y_test),q)\n",
    "for i,y in enumerate(y_test):\n",
    "    y_test_p[i,:] = torch.cat([y,torch.zeros(q-y.shape[0])])\n",
    "test_data = TensorDataset(x_test_n.T,y_test_p.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(train_data, 'train_data_int_comp.pt') \n",
    "# torch.save(test_data, 'test_data_int_comp.pt') \n",
    "train_data = torch.load('./Saved Data\\\\train_data_int_comp.pt')\n",
    "#test_data = torch.load('test_data_int_comp.pt')\n",
    "\n",
    "for i in train_data:\n",
    "    size_vec=i[1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterators creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader, valid_idx, train_idx = dataset_loader(train_data,test_data, batch_size = 3000, valid_size = 0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture \n",
    "Here, we explore a classical seq2seq model mapping numerical values to equations in a supervised manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "The encoder is the original component of our architecture. In this case, it has to take as input the numerical values of a certain equation. First set of experiments will test the y-only input. Then, we will evaluate the performances when x,y pairs are given. \n",
    "Since the input to the encoder is not a sequence of symbols, the classical embedding based architecture cannot be used. We therefore replace the embedding layer by a linear and a convolutional term respectively. More advanced solution will be investigated later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, emb_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [batch size,src len]\n",
    "        \n",
    "        src = torch.unsqueeze(src.T,dim = 2)\n",
    "        \n",
    "        #src = [src len, batch size,1]\n",
    "        \n",
    "        embedded = self.dropout(self.relu(self.linear(src)))\n",
    "\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "The decoder follows the standard pipeline of NMT tasks. An embedding layer is used to map discrete tokenization to real vectors. Teacher forcing is used with a fixed probability value. The decoder is based on autoregression, i.e. the output of time step t is the output of time setp t+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(self.relu(output.squeeze(0)))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        trg = trg.T\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select hyperparameters and trainign setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,040,088 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "INPUT_DIM = 1\n",
    "#OUTPUT_DIM = 16 # basiscs\n",
    "OUTPUT_DIM = 16 # N_terms = 1\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 500\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip,noise_Y=False,sigma = 0.1):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        if noise_Y:\n",
    "            src = batch[0] + torch.from_numpy(sigma*np.random.randn(batch[0].shape[0],30)).float().cuda()\n",
    "            trg = batch[1]\n",
    "        else:\n",
    "            src = batch[0]\n",
    "            trg = batch[1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg.T[1:].contiguous().view(-1)\n",
    "        #trg = trg.T[1:].reshape(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch[0]\n",
    "            trg = batch[1]\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg.T[1:].contiguous().view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(x_train_n.T[19,:] + 0.1*np.random.randn(30))\n",
    "# plt.plot(x_train_n.T[19,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Seq2Seq:\n\tsize mismatch for encoder.linear.weight: copying a param with shape torch.Size([256, 1]) from checkpoint, the shape in current model is torch.Size([4, 1]).\n\tsize mismatch for encoder.linear.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([4]).\n\tsize mismatch for encoder.rnn.weight_ih_l0: copying a param with shape torch.Size([2000, 256]) from checkpoint, the shape in current model is torch.Size([2000, 4]).\n\tsize mismatch for decoder.embedding.weight: copying a param with shape torch.Size([16, 256]) from checkpoint, the shape in current model is torch.Size([16, 4]).\n\tsize mismatch for decoder.rnn.weight_ih_l0: copying a param with shape torch.Size([2000, 256]) from checkpoint, the shape in current model is torch.Size([2000, 4]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-96e5c38b03be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mN_EPOCHS\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mCLIP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'internals_and_comp_noise_model1.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbest_valid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'inf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\New_eq_learn\\ELEARN\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    845\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m--> 847\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m    848\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Seq2Seq:\n\tsize mismatch for encoder.linear.weight: copying a param with shape torch.Size([256, 1]) from checkpoint, the shape in current model is torch.Size([4, 1]).\n\tsize mismatch for encoder.linear.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([4]).\n\tsize mismatch for encoder.rnn.weight_ih_l0: copying a param with shape torch.Size([2000, 256]) from checkpoint, the shape in current model is torch.Size([2000, 4]).\n\tsize mismatch for decoder.embedding.weight: copying a param with shape torch.Size([16, 256]) from checkpoint, the shape in current model is torch.Size([16, 4]).\n\tsize mismatch for decoder.rnn.weight_ih_l0: copying a param with shape torch.Size([2000, 256]) from checkpoint, the shape in current model is torch.Size([2000, 4])."
     ]
    }
   ],
   "source": [
    "N_EPOCHS =50\n",
    "CLIP = 1\n",
    "model.load_state_dict(torch.load('internals_and_comp_noise_model1.pt'))\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP,noise_Y = True,sigma = 0.05)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'internals_and_comp_noise_model1.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    cnt = 0\n",
    "    cond = True\n",
    "    while cond == True:\n",
    "        string, dictionary =  fun_generator.generate_batch(support,1, X_noise=False, Y_noise=0)\n",
    "        if np.all(string[0][1] == 0) == False:\n",
    "            if np.max(string[0][1])<1000 and np.min(string[0][1])>-1000 and tokenization.get_string(tokenization.pipeline(dictionary)[0])[-1] != '+': \n",
    "                x_test.append(string[0][1])\n",
    "                y_test.append(torch.Tensor((tokenization.pipeline(dictionary)[0])))\n",
    "                #print(tokenization.get_string(tokenization.pipeline(dictionary)[0]))\n",
    "                cnt+=1\n",
    "        if cnt == 500:\n",
    "            cond = False\n",
    "    scaler = MinMaxScaler()\n",
    "    x_test_n = scaler.fit_transform(np.array(x_test).T)\n",
    "    x_test_n = torch.Tensor(x_test_n)\n",
    "    l = [len(y) for y in y_test]\n",
    "    q = np.max(l)\n",
    "    y_test_p = torch.zeros(len(y_test),q)\n",
    "    for i,y in enumerate(y_test):\n",
    "        y_test_p[i,:] = torch.cat([y,torch.zeros(q-y.shape[0])])\n",
    "    test_data = TensorDataset(x_test_n.T,y_test_p.long())\n",
    "    \n",
    "    train_loader, valid_loader, test_loader, valid_idx, train_idx = dataset_loader(train_data,test_data, batch_size = 3000, valid_size = 0.30)\n",
    "\n",
    "    model.load_state_dict(torch.load('internals_and_comp_noise_model1.pt'))\n",
    "\n",
    "    test_loss = evaluate(model, test_loader, criterion)\n",
    "\n",
    "    print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_evaluation(model_o,y_test,n):\n",
    "    c = True\n",
    "    i=0\n",
    "    while c:\n",
    "        try:\n",
    "            fun = lambdify(x,tokenization.get_string(np.array([12]+list(model_o.detach().argmax(2)[1:-i,n].cpu()))))\n",
    "            print('Prediction: ', tokenization.get_string(np.array([12]+list(model_o.detach().argmax(2)[1:-i,n].cpu()))))\n",
    "            c = False\n",
    "        except:\n",
    "            i+=1\n",
    "    print('Ground Truth: ', tokenization.get_string(y_test[n].numpy()))\n",
    "    trueth = lambdify(x,tokenization.get_string(y_test[n].numpy()))\n",
    "    enlarged_supp = np.arange(0.1,15,0.1)\n",
    "    supp = np.arange(0.1,3.1,0.1)\n",
    "    MSE = np.mean((trueth(enlarged_supp)-fun(enlarged_supp))**2)\n",
    "    print('MSE: ', MSE)\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.plot(enlarged_supp,trueth(enlarged_supp), label = 'ground-true')\n",
    "    plt.plot(enlarged_supp,fun(enlarged_supp), label = 'prediction')\n",
    "    plt.plot(supp,x_test[n], label = 't')\n",
    "    plt.legend()\n",
    "#    plt.ylim(0,10)\n",
    "#    plt.xlim(0,3)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    a = model(x_test_n.T[:1000].cuda(), y_test_p[:1000].long().cuda(),0)\n",
    "num_evaluation(a,y_test,199)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_eval(support, model, y_test_p):\n",
    "    model.load_state_dict(torch.load('internals_and_comp_noise_model1.pt'))\n",
    "    model.eval()\n",
    "    expression = input()\n",
    "    av = lambdify(x,expression)\n",
    "    xxx = av(support)\n",
    "    scaler = MinMaxScaler()\n",
    "    xxx_n = scaler.fit_transform(np.expand_dims(xxx,0).T)\n",
    "    #print(xxx_n.T)\n",
    "    x_new = torch.from_numpy(xxx_n.T).float().cuda()\n",
    "    with torch.no_grad():\n",
    "        pred = model(x_new,y_test_p[:1].long().cuda(),0)\n",
    "        print(pred.shape)\n",
    "        print(tokenization.get_string(np.array([12]+list(pred.detach().argmax(2)[1:].cpu()))))\n",
    "        c = True\n",
    "        i=0\n",
    "        while c:\n",
    "            try:\n",
    "                fun = lambdify(x,tokenization.get_string(np.array([12]+list(pred.detach().argmax(2)[1:-i].cpu()))))\n",
    "                print('Prediction: ', tokenization.get_string(np.array([12]+list(pred.detach().argmax(2)[1:-i].cpu()))))\n",
    "                c = False\n",
    "            except:\n",
    "                i+=1\n",
    "        print('Ground Truth: ', expression)\n",
    "        ss = np.arange(0.1,15.1,0.1)\n",
    "        enlarged_supp = np.arange(0.1,3.1,0.1)\n",
    "        MSE = np.mean((av(enlarged_supp)-fun(enlarged_supp))**2)\n",
    "        print('MSE: ', MSE)\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.plot(enlarged_supp,scaler.fit_transform(np.expand_dims(av(enlarged_supp),1))[:,0], label = 'ground-true')\n",
    "        plt.plot(enlarged_supp,scaler.fit_transform(np.expand_dims(fun(enlarged_supp),1))[:,0], label = 'prediction')\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.plot(ss,(np.expand_dims(av(ss),1))[:,0], label = 'ground-true')\n",
    "        plt.plot(ss,(np.expand_dims(fun(ss),1))[:,0], label = 'prediction')\n",
    "        plt.legend()\n",
    "        #plt.ylim(-4,10)\n",
    "        #print(scaler.fit_transform(np.expand_dims(fun(support),1)).T)\n",
    "        return pred\n",
    "\n",
    "prob = user_eval(support, model,size_vec.unsqueeze(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantify training test overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test =[]\n",
    "for j in range(len(y_test)):\n",
    "    print(j)\n",
    "    w=0\n",
    "    for i in range(len(y_train)):\n",
    "        try:\n",
    "            if torch.all(y_test[j] == y_train[i]):\n",
    "                w = 1\n",
    "                break\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "    if w == 0:\n",
    "        final_test.append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(final_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(0.5*x)*2 + sin(2*x) tokenization.get_string(y_test[final_test[31]].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. provare le due normalizzazione separatamente\n",
    "\n",
    "2. introdurre misure per misurare generalizzazione ootraining set\n",
    "\n",
    "3. mettere noise\n",
    "\n",
    "4. cnn instead of rnn\n",
    "\n",
    "5. capire l'output  nel training set\n",
    "\n",
    "6. usare come val loss curve mai viste\n",
    "\n",
    "7. quantificare livello di generalizzazione\n",
    "    \n",
    "    7.1 griglia fissa di dati, data structure , numero di dati, normalizzazione\n",
    "    \n",
    "    7.2 iterativit\n",
    "\n",
    "8. curriculum learning\n",
    "\n",
    "9. Siren: usa embedding as encoder---> vantaggioso per normalizzare \n",
    "\n",
    "10. Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "input = torch.randn(3, 5)\n",
    "print(input)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "print(output)\n",
    "# >>> output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.log(np.exp(input.numpy()[0,4])/np.sum(np.exp(input.numpy()[0,:]))) + np.log(np.exp(input.numpy()[1,3])/np.sum(np.exp(input.numpy()[1,:]))) + np.log(np.exp(input.numpy()[2,0])/np.sum(np.exp(input.numpy()[2,:]))))/3 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ELEARN",
   "language": "python",
   "name": "elearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
